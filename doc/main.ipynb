{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to gsimmons17@gsb.columbia.edu and will expire on December 07, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1493066255.log\n"
     ]
    }
   ],
   "source": [
    "msf = gl.load_sframe('../data/kindle_data.sf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime',\n",
       " 'upvotes',\n",
       " 'downvotes',\n",
       " 'tfidf',\n",
       " 'brand',\n",
       " 'categories',\n",
       " 'description',\n",
       " 'imUrl',\n",
       " 'price',\n",
       " 'related',\n",
       " 'salesRank',\n",
       " 'title']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msf.column_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an SArray of the concatenated text in the `summary`, `reviewText`, and `description` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = msf.apply(lambda x: str(x['summary']) + ' ' + str(x['reviewText']) + ' ' + str(x['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to count words from a `docs` SArray that outputs a `docs_sf` SFrame with associated word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_frequency(docs):\n",
    "    \"\"\"\n",
    "    Returns the frequency of occurrence of words in an SArray of documents\n",
    "    Args:\n",
    "    docs: An SArray (of dtype str) of documents\n",
    "    Returns:\n",
    "    An SFrame with the following columns:\n",
    "     'word'      : Word used\n",
    "     'count'     : Number of times the word occured in all documents.\n",
    "     'frequency' : Relative frequency of the word in the set of input documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the count_words function to count the number of words.\n",
    "    docs_sf = gl.SFrame()\n",
    "    docs_sf['words'] = gl.text_analytics.count_words(docs)\n",
    "\n",
    "    # Stack the dictionary into individual word-count pairs.\n",
    "    docs_sf = docs_sf.stack('words', \n",
    "                         new_column_name=['word', 'count'])\n",
    "\n",
    "    # Count the number of unique words (remove None values)\n",
    "    docs_sf = docs_sf.groupby('word', {'count': gl.aggregate.SUM('count')})\n",
    "    docs_sf['frequency'] = docs_sf['count'] / docs_sf[\"count\"].sum()\n",
    "    return docs_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_sf = get_word_frequency(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(document_bow, word_topic_counts, topic_counts, vocab,\n",
    "            alpha=0.1, beta=0.01, num_burnin=5):\n",
    "    \"\"\"\n",
    "    Make predictions for a single document.\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_bow : dict\n",
    "        Dictionary with words as keys and document frequencies as counts.\n",
    "    word_topic_counts : numpy array, num_vocab x num_topics\n",
    "        Number of times a given word has ever been assigned to a topic.\n",
    "    topic_counts : numpy vector of length num_topics\n",
    "        Number of times any word has been assigned to a topic.\n",
    "    vocab : dict\n",
    "        Words are keys and unique integer is the value.\n",
    "    alpha : float\n",
    "        Hyperparameter. See topic_model docs.\n",
    "    beta : float\n",
    "        Hyperparameter. See topic_model docs.\n",
    "    num_burnin : int\n",
    "        Number of iterations of Gibbs sampling to perform at predict time.\n",
    "    Returns\n",
    "    -------\n",
    "    out : numpy array of length num_topics\n",
    "        Probabilities that the document belongs to each topic.\n",
    "    \"\"\"\n",
    "    num_vocab, num_topics = word_topic_counts.shape\n",
    "\n",
    "    # proportion of each topic in this test doc\n",
    "    doc_topic_counts = np.zeros(num_topics)\n",
    "    # Assignment of each unique word\n",
    "    doc_topic_assignments = []\n",
    "\n",
    "    # Initialize assignments and counts\n",
    "    # NB: we are assuming document_bow doesn't change.\n",
    "    for i, (word, freq) in enumerate(document_bow.iteritems()):\n",
    "        if word not in vocab:  # skip words not present in training set\n",
    "            continue\n",
    "        topic = np.random.randint(0, num_topics-1)\n",
    "        doc_topic_assignments.append(topic)\n",
    "        doc_topic_counts[topic] += freq\n",
    "\n",
    "    # Sample topic assignments for the test document\n",
    "    for burnin in range(num_burnin):\n",
    "        for i, (word, freq) in enumerate(document_bow.iteritems()):\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            word_id = vocab[word]\n",
    "\n",
    "            # Get old topic and decrement counts\n",
    "            topic = doc_topic_assignments[i]\n",
    "            doc_topic_counts[topic] -= freq\n",
    "\n",
    "            # Sample a new topic\n",
    "            gamma = np.zeros(num_topics)  # store probabilities\n",
    "            for k in range(num_topics):\n",
    "                gamma[k] = (doc_topic_counts[k] + alpha) * (word_topic_counts[word_id, k] + beta) / (topic_counts[k] + num_vocab * beta)\n",
    "            gamma = gamma / gamma.sum()  # normalize to probabilities\n",
    "            topic = np.random.choice(num_topics, 1, p=gamma)\n",
    "\n",
    "            # Use new topic to increment counts\n",
    "            doc_topic_assignments[i] = topic\n",
    "            doc_topic_counts[topic] += freq\n",
    "\n",
    "    # Create predictions\n",
    "    predictions = np.zeros(num_topics)\n",
    "    total_doc_topic_counts = doc_topic_counts.sum()\n",
    "    for k in range(num_topics):\n",
    "        predictions[k] = (doc_topic_counts[k] + alpha) / (total_doc_topic_counts + num_topics * alpha)\n",
    "    return predictions / predictions.sum()\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "    docs = gl.SFrame({'text': [{'first': 5, 'doc': 1}, {'second': 3, 'doc': 5}]})\n",
    "    m = gl.topic_model.create(docs)\n",
    "\n",
    "    # Get test document in bag of words format\n",
    "    document_bow = docs['text'][0]\n",
    "\n",
    "    # Input: Global parameters from trained model\n",
    "\n",
    "    # Number of times each word in the vocabulary has ever been assigned to topic k (in any document). You can make an approximate version of this by multiplying m['topics'] by some large number (e.g. number of tokens in corpus) that indicates how strong you \"believe\" in these topics. Make it into counts by flooring it to an integer.\n",
    "    prior_strength = 1000000\n",
    "    word_topic_counts = np.array(m['topics']['topic_probabilities'])\n",
    "    word_topic_counts = np.floor(prior_strength * word_topic_counts)\n",
    "\n",
    "    # Number of times any word as been assigned to each topic.\n",
    "    topic_counts = word_topic_counts.sum(0)\n",
    "\n",
    "    # Get vocabulary lookup\n",
    "    num_topics = m['num_topics']\n",
    "    vocab = {}\n",
    "    for i, w in enumerate(m['topics']['vocabulary']):\n",
    "        vocab[w] = i\n",
    "    num_vocab = len(vocab)\n",
    "\n",
    "    # Make prediction on test document\n",
    "    probs = predict(document_bow, word_topic_counts, topic_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay for true beginners\n",
      "So, I bought this book a few days ago and have tried three recipes so far.  The first was a total flop.  There must be an error, but be forewarned, do NOT make the Blueberry Coffee Cake as it comes out as inedible mush--WAY too much water.  The other two recipes (mac and cheese and grilled cheese with tomato) were decent for quick lunches or dinners.  They were average in taste, but considering the short amount of time it took to make them, I'm okay with that.  All in all, it's a nice idea book to get creative with everyday ingredients, but with errors and only average taste, I give it three stars.\n",
      "In less time and for less money than it takes to order pizza, you can make it yourself!Three harried but heatlh-conscious college students compiled and tested this collection of more than 200 tasty, hearty, inexpensive recipes anyone can cook -- yes, anyone!Whether you're short on cash, fearful of fat, counting your calories, or just miss home cooking, The Healthy College Cookbook offers everything you need to make good food yourself.\n"
     ]
    }
   ],
   "source": [
    "print msf[1]['summary']\n",
    "print msf[1]['reviewText']\n",
    "print msf[1]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = gl.text_analytics.count_words(docs, to_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ToolkitError",
     "evalue": "Input data is not an SFrame. If it is a Pandas DataFrame, you may use the to_sframe() function to convert it to an SFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mToolkitError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0457887f7cbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit and transform the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtransformed_sf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/galen/anaconda/envs/gl-env/lib/python2.7/site-packages/graphlab/toolkits/feature_engineering/_doc_utils.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrepublish_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/galen/anaconda/envs/gl-env/lib/python2.7/site-packages/graphlab/toolkits/feature_engineering/_feature_engineering.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0m_raise_error_if_not_sframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0m_mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metric_tracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__proxy__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/galen/anaconda/envs/gl-env/lib/python2.7/site-packages/graphlab/toolkits/_internal_utils.pyc\u001b[0m in \u001b[0;36m_raise_error_if_not_sframe\u001b[0;34m(dataset, variable_name)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mToolkitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvariable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_error_if_sframe_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SFrame\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mToolkitError\u001b[0m: Input data is not an SFrame. If it is a Pandas DataFrame, you may use the to_sframe() function to convert it to an SFrame."
     ]
    }
   ],
   "source": [
    "trimmer = gl.toolkits.feature_engineering.RareWordTrimmer(threshold=2)\n",
    "\n",
    "# Fit and transform the data.\n",
    "transformed_sf = trimmer.fit_transform(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3205467"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the count_words function to count the number of words.\n",
    "docs_sf = gl.SFrame()\n",
    "docs_sf['words'] = gl.text_analytics.count_words(docs)\n",
    "\n",
    "# Stack the dictionary into individual word-count pairs.\n",
    "docs_sf = docs_sf.stack('words', \n",
    "                     new_column_name=['word', 'count'])\n",
    "\n",
    "# Count the number of unique words (remove None values)\n",
    "docs_sf = docs_sf.groupby('word', {'count': gl.aggregate.SUM('count')})\n",
    "docs_sf['frequency'] = docs_sf['count'] / docs_sf[\"count\"].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
